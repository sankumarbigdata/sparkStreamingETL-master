application {
  checkpointLocation = "file:///home/stefano/Documents/SPARKINGEST/checkpoints"
}

sources {

  sourceToApply = "joinables"

  //example of configuration for csv files
  file {
    required {
      path = "/home/stefano/Documents/SPARKINGEST/*.csv"
      format = "csv"
    }
    optional {
      refreshTime = 1 //check every X ms for new files
      sep = ","
    }
    //IF SCHEMA IS PRESENT, IT WILL BE TAKEN
    schema = [
      {name: string},
      {surname: string}
      {age: int}
    ]
  }

  joinables {

    toJoin = ["file1", "file2"]
    joinConditions = ["name = namerino AND sparkProcessTime >=  processTime AND sparkProcessTime <= processTime + INTERVAL 1 HOUR"]

    file1 {
      sourceType = "file"
      required {
        path = "/home/stefano/Documents/SPARKINGEST/*.csv"
        format = "csv"
      }
      optional {
        refreshTime = 1 //check every X ms for new files
        sep = ","
      }
      //IF SCHEMA IS PRESENT, IT WILL BE TAKEN
      schema = [
        {name: string},
        {surname: string}
        {age: int}
      ]

      transformations = ["date"]
      order = ["date"]

      date {
        column = "processTime"
      }

    }

    file2 {
      sourceType = "file"
      required {
        path = "/home/stefano/pathAlternativo/*.csv"
        format = "csv"
      }
      optional {
        refreshTime = 1 //check every X ms for new files
        sep = ","
      }
      //IF SCHEMA IS PRESENT, IT WILL BE TAKEN
      schema = [
        {namerino: string},
        {surname: string}
        {age: int}
      ]

      transformations = ["date"]
      order = ["date"]

      date {
        column = "sparkProcessTime"
      }
    }

    kafka2 {
      sourceType = "kafka"
      kafka-bootstrap-servers = "localhost:9092" //comma separated list

      required {
        subscribe = "ingestion" //topic(s) to subscribe to
      }
      optional {
        startingOffsets = "earliest" //earliest: from start of kafka topic, latest: only new data from now.
      }
    }
  }

  kafka {
    kafka-bootstrap-servers = "localhost:9092" //comma separated list

    required {
      subscribe = "ingestion" //topic(s) to subscribe to
    }
    optional {
      startingOffsets = "earliest" //earliest: from start of kafka topic, latest: only new data from now.
    }
  }

}

sinks {

  sinkToApply = "console"

  file {
    required {
      path = "file:///home/stefano/Documents/SPARKINGEST"
      format = "json"
    }
    optional {
      outputMode = "append"
    }
  }

  //writes whole row to kafka topic as JSON
  kafka {
    kafka-bootstrap-servers = "localhost:9092"

    required {
      topic = "enriched"
    }
    optional {}
  }

  console {
    required {}
    optional {
      outputMode = "complete"
    }
  }

  es {
    index = "spark"
    type = "people"

    required {}
    optional {}
  }
}

transformations {
  //defined order of transformations to apply
  order = ["date", "watermark", "add"]

  date {
    column = "sparkProcessTime"
  }

  watermark {
    column = "sparkProcessTime"
    duration = "10 minute"
  }

  add {
    columns {

      dummyNum1 {
        value = "1"
      }


      dummyNum2 {
        value = "2"
      }

    }
  }

  filter = [
    "name LIKE 'stefano%'"
  ]
  select = [
    "sparkProcessTime", "name"
  ]

  remove {
    columns = []
  }

  count {
    watermarkColumn = "sparkProcessTime"
    windowTime = "2 minute"
    slidingTime = "1 minute"
    columns = ["dummyNum1"]
  }

}

listeners {

  listenersToApply = ["file"]

  kafka {
    topic = "metrics"
    servers = ${sources.kafka.kafka-bootstrap-servers}
    keySerializer = "org.apache.kafka.common.serialization.StringSerializer"
    valueSerializer = "org.apache.kafka.common.serialization.StringSerializer"
  }

  file {
    filename = "/home/stefano/Documents/SPARKINGEST/sparkstream.log"
  }

}